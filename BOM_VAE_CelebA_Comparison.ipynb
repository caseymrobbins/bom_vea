{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOM-VAE vs Œ≤-VAE Comparison on CelebA\n",
    "\n",
    "**Hypothesis**: BOM achieves comparable or better results than Œ≤-VAE without requiring hyperparameter tuning.\n",
    "\n",
    "**Adaptive squeeze rule**:\n",
    "```\n",
    "squeeze_amount = (s_min - 0.5) * k\n",
    "```\n",
    "- When s_min = 0.9: squeeze aggressively\n",
    "- When s_min = 0.55: squeeze gently\n",
    "- When s_min ‚â§ 0.5: stop squeezing\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Auto-recovery from KL explosions\n",
    "- ‚úÖ Detailed violation reporting\n",
    "- ‚úÖ Multiple CelebA download options\n",
    "- ‚úÖ Conservative calibration with safety margins\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Select GPU Runtime**: Runtime ‚Üí Change runtime type ‚Üí **L4 GPU** (or T4/A100)\n",
    "2. **Run all cells** in order\n",
    "3. **Total runtime**: ~3-4 hours on L4, ~1.5 hours on A100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown -q\n",
    "\n",
    "import os, zipfile, glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import gdown\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download CelebA Dataset\n",
    "\n",
    "**Multiple download options:**\n",
    "- **Option 1**: Automatic download (may fail due to rate limits)\n",
    "- **Option 2**: Manual Kaggle download (most reliable)\n",
    "- **Option 3**: Google Drive manual\n",
    "\n",
    "See error messages below for instructions if automatic download fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_path = '/content/celeba'\n",
    "zip_path = '/content/celeba.zip'\n",
    "\n",
    "if not os.path.exists(celeba_path) or len(os.listdir(celeba_path)) == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CELEBA DATASET REQUIRED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nOption 1: Download from Kaggle (RECOMMENDED)\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"1. Go to: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\")\n",
    "    print(\"2. Click 'Download' (requires Kaggle account)\")\n",
    "    print(\"3. Upload 'archive.zip' to Colab\")\n",
    "    print(\"4. Run: !unzip -q /content/archive.zip -d /content/celeba\")\n",
    "    print(\"\\nOption 2: Try automatic download (may fail due to rate limits)\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(\"Attempting automatic download...\")\n",
    "            url = \"https://drive.google.com/file/d/1xJs_8JB0HYXiaAmU8PTG9qbk0WJ2Wo1U/view?usp=sharing\"\n",
    "            gdown.download(url, zip_path, quiet=False, fuzzy=True)\n",
    "\n",
    "        if os.path.exists(zip_path):\n",
    "            print(\"Extracting dataset...\")\n",
    "            os.makedirs(celeba_path, exist_ok=True)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(celeba_path)\n",
    "            print(\"‚úì Extraction complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Automatic download failed: {e}\")\n",
    "        print(\"\\nPlease use Kaggle method (recommended):\")\n",
    "        print(\"1. Download from https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\")\n",
    "        print(\"2. Upload archive.zip to Colab\")\n",
    "        print(\"3. Run: !unzip -q /content/archive.zip -d /content/celeba\")\n",
    "        print(\"4. Re-run this cell\")\n",
    "        raise RuntimeError(\"CelebA dataset not found. Please download manually.\")\n",
    "\n",
    "num_images = len(glob.glob(f\"{celeba_path}/**/*.jpg\", recursive=True))\n",
    "if num_images == 0:\n",
    "    raise RuntimeError(\"No images found. Check dataset structure.\")\n",
    "\n",
    "print(f\"\\n‚úì Found {num_images:,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 256*4*4)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, 2, 1, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 2, 1, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.BatchNorm2d(32), nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 3, 3, 2, 1, 1), nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.enc(x).view(x.size(0), -1)\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
    "        return self.dec(self.fc_dec(z).view(-1, 256, 4, 4)), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_celeba(data_path, batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(178),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    if os.path.basename(data_path) == 'img_align_celeba':\n",
    "        data_path = os.path.dirname(data_path)\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    print(f\"‚úì Loaded CelebA: {len(dataset):,} images, {len(loader)} batches\")\n",
    "    return loader\n",
    "\n",
    "# Adjust BATCH_SIZE: L4=128, A100=256, T4=64\n",
    "BATCH_SIZE = 128\n",
    "train_loader = load_celeba(celeba_path, batch_size=BATCH_SIZE)\n",
    "test_loader = load_celeba(celeba_path, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x, x_recon, mu, logvar):\n",
    "    B = x.size(0)\n",
    "    mse = F.mse_loss(x_recon, x, reduction='none').view(B, -1).mean(1)\n",
    "    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
    "    dx = torch.abs(x_recon[:,:,:,1:] - x_recon[:,:,:,:-1])\n",
    "    dy = torch.abs(x_recon[:,:,1:,:] - x_recon[:,:,:-1,:])\n",
    "    sharp = (dx.mean([1,2,3]) + dy.mean([1,2,3])) / 2\n",
    "    return mse, kl, sharp\n",
    "\n",
    "def evaluate(model, loader, device, max_batches=100):\n",
    "    model.eval()\n",
    "    all_mse, all_kl, all_sharp = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= max_batches: break\n",
    "            x = batch[0].to(device)\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            mse, kl, sharp = compute_metrics(x, x_recon, mu, logvar)\n",
    "            all_mse.extend(mse.cpu().numpy())\n",
    "            all_kl.extend(kl.cpu().numpy())\n",
    "            all_sharp.extend(sharp.cpu().numpy())\n",
    "    return {'mse': np.mean(all_mse), 'kl': np.mean(all_kl), 'sharp': np.mean(all_sharp)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Œ≤-VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_beta_vae(model, loader, device, beta, n_epochs=20):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(loader, desc=f\"Œ≤-VAE (Œ≤={beta}) Epoch {epoch}/{n_epochs}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            mse, kl, sharp = compute_metrics(x, x_recon, mu, logvar)\n",
    "            loss = mse.mean() + beta * kl.mean()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            history.append({'mse': mse.mean().item(), 'kl': kl.mean().item(), 'sharp': sharp.mean().item()})\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'kl': f\"{kl.mean().item():.0f}\"})\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. BOM-VAE with Auto-Recovery\n",
    "\n",
    "Key improvements:\n",
    "- ‚úÖ Detailed violation reporting (shows which constraint failed)\n",
    "- ‚úÖ Auto-recovery if early epochs have >20% violations\n",
    "- ‚úÖ Conservative initial calibration\n",
    "- ‚úÖ Intelligent constraint widening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_constraint_lower_better(value, floor):\n",
    "    return (floor - value) / floor\n",
    "\n",
    "def regular_constraint_higher_better(value, ceiling):\n",
    "    return value / ceiling\n",
    "\n",
    "def box_constraint(value, floor_low, optimum, floor_high):\n",
    "    left = (value - floor_low) / (optimum - floor_low)\n",
    "    right = (floor_high - value) / (floor_high - optimum)\n",
    "    return torch.minimum(left, right)\n",
    "\n",
    "def compute_bom_loss(x, x_recon, mu, logvar, mse_floor, kl_floor_low, kl_optimum, kl_floor_high, sharp_ceiling):\n",
    "    \"\"\"Compute BOM loss with detailed violation reporting.\"\"\"\n",
    "    mse, kl, sharp = compute_metrics(x, x_recon, mu, logvar)\n",
    "    mse_score = regular_constraint_lower_better(mse, mse_floor)\n",
    "    kl_score = box_constraint(kl, kl_floor_low, kl_optimum, kl_floor_high)\n",
    "    sharp_score = regular_constraint_higher_better(sharp, sharp_ceiling)\n",
    "    scores = torch.stack([mse_score, kl_score, sharp_score], dim=1)\n",
    "    s_min, min_idx = torch.min(scores, dim=1)\n",
    "\n",
    "    metrics = {\n",
    "        'mse': mse.mean().item(), 'kl': kl.mean().item(), 'sharp': sharp.mean().item(),\n",
    "        'mse_max': mse.max().item(), 'kl_max': kl.max().item(), 'kl_min': kl.min().item(),\n",
    "        'sharp_min': sharp.min().item(), 's_min': s_min.mean().item(),\n",
    "        'violations': (s_min <= 0).sum().item(),\n",
    "        'mse_violations': (mse_score <= 0).sum().item(),\n",
    "        'kl_violations': (kl_score <= 0).sum().item(),\n",
    "        'sharp_violations': (sharp_score <= 0).sum().item(),\n",
    "    }\n",
    "\n",
    "    if metrics['violations'] > 0:\n",
    "        return None, metrics\n",
    "\n",
    "    loss = -torch.log(s_min).mean()\n",
    "    names = ['mse', 'kl', 'sharp']\n",
    "    metrics['bottleneck'] = names[torch.bincount(min_idx, minlength=3).argmax().item()]\n",
    "    metrics['loss'] = loss.item()\n",
    "    return loss, metrics\n",
    "\n",
    "def calibrate_bom(model, loader, device, n_batches=50):\n",
    "    model.train()\n",
    "    all_mse, all_kl, all_sharp = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= n_batches: break\n",
    "            x = batch[0].to(device)\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            mse, kl, sharp = compute_metrics(x, x_recon, mu, logvar)\n",
    "            all_mse.extend(mse.cpu().numpy())\n",
    "            all_kl.extend(kl.cpu().numpy())\n",
    "            all_sharp.extend(sharp.cpu().numpy())\n",
    "\n",
    "    mse_arr, kl_arr, sharp_arr = np.array(all_mse), np.array(all_kl), np.array(all_sharp)\n",
    "    params = {\n",
    "        'mse_floor': mse_arr.max() * 2.0,\n",
    "        'kl_floor_low': max(kl_arr.min() * 0.1, 0.1),\n",
    "        'kl_optimum': kl_arr.mean(),\n",
    "        'kl_floor_high': kl_arr.max() * 100.0,  # Very loose initially\n",
    "        'sharp_ceiling': sharp_arr.mean() * 0.5,\n",
    "    }\n",
    "    print(f\"Calibration: MSE={mse_arr.mean():.4f} (max={mse_arr.max():.4f}), KL={kl_arr.mean():.1f} (range=[{kl_arr.min():.1f}, {kl_arr.max():.1f}])\")\n",
    "    print(f\"Initial constraints: mse_floor={params['mse_floor']:.4f}, kl_box=[{params['kl_floor_low']:.1f}, {params['kl_optimum']:.1f}, {params['kl_floor_high']:.1f}]\\n\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_bom_vae(model, loader, device, n_epochs=20):\n    \"\"\"BOM-VAE training with auto-recovery from early failures.\"\"\"\n    params = calibrate_bom(model, loader, device)\n    mse_floor = params['mse_floor']\n    kl_floor_low = params['kl_floor_low']\n    kl_optimum = params['kl_optimum']\n    kl_floor_high = params['kl_floor_high']\n    sharp_ceiling = params['sharp_ceiling']\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    history = []\n\n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        epoch_loss, epoch_s_min = [], []\n        epoch_violations = 0\n        epoch_mse_violations, epoch_kl_violations, epoch_sharp_violations = 0, 0, 0\n        epoch_mse_values, epoch_kl_values, epoch_sharp_values = [], [], []\n\n        pbar = tqdm(loader, desc=f\"BOM-VAE Epoch {epoch}/{n_epochs}\")\n        for batch in pbar:\n            x = batch[0].to(device)\n            optimizer.zero_grad()\n            x_recon, mu, logvar = model(x)\n            loss, metrics = compute_bom_loss(x, x_recon, mu, logvar, mse_floor, kl_floor_low, kl_optimum, kl_floor_high, sharp_ceiling)\n\n            if loss is not None:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                epoch_loss.append(metrics['loss'])\n                epoch_s_min.append(metrics['s_min'])\n            else:\n                epoch_violations += metrics['violations']\n                epoch_mse_violations += metrics['mse_violations']\n                epoch_kl_violations += metrics['kl_violations']\n                epoch_sharp_violations += metrics['sharp_violations']\n\n            epoch_mse_values.append(metrics['mse'])\n            epoch_kl_values.append(metrics['kl'])\n            epoch_sharp_values.append(metrics['sharp'])\n            history.append(metrics)\n            pbar.set_postfix({'s_min': f\"{metrics['s_min']:.3f}\", 'kl': f\"{metrics['kl']:.0f}\", 'viol': epoch_violations})\n\n        avg_s_min = np.mean(epoch_s_min) if epoch_s_min else 0\n        violation_rate = epoch_violations / (len(loader) * loader.batch_size)\n\n        print(f\"  Epoch {epoch}: s_min={avg_s_min:.3f}, violations={epoch_violations} ({violation_rate*100:.1f}%)\")\n        print(f\"    Values: MSE={np.mean(epoch_mse_values):.4f} (max={np.max(epoch_mse_values):.4f}), KL={np.mean(epoch_kl_values):.1f} (range=[{np.min(epoch_kl_values):.1f}, {np.max(epoch_kl_values):.1f}])\")\n\n        if epoch_violations > 0:\n            print(f\"    ‚ö†Ô∏è  Violations: MSE={epoch_mse_violations}, KL={epoch_kl_violations}, Sharp={epoch_sharp_violations}\")\n\n        # Auto-recovery for early epochs with >20% violations\n        if epoch <= 3 and violation_rate > 0.2:\n            print(f\"    üîß AUTO-RECOVERY: {violation_rate*100:.1f}% violations in early epoch\")\n            if epoch_mse_violations > 0:\n                mse_floor = max(np.max(epoch_mse_values) * 3.0, mse_floor * 1.5)\n                print(f\"       MSE: Widened floor to {mse_floor:.4f}\")\n            if epoch_kl_violations > 0:\n                kl_floor_high = max(np.max(epoch_kl_values) * 2.0, kl_floor_high * 2.0)\n                print(f\"       KL: Widened high bound to {kl_floor_high:.1f}\")\n                if np.min(epoch_kl_values) < kl_floor_low:\n                    kl_floor_low = max(np.min(epoch_kl_values) * 0.5, 0.1)\n                    print(f\"       KL: Widened low bound to {kl_floor_low:.1f}\")\n            if epoch_sharp_violations > 0:\n                sharp_ceiling = np.min(epoch_sharp_values) * 0.8\n                print(f\"       Sharp: Lowered ceiling to {sharp_ceiling:.4f}\")\n\n        # Adaptive squeeze (gentler: starts epoch 5, k=0.3)\n        if epoch >= 5 and avg_s_min > 0.5 and violation_rate < 0.1:\n            squeeze_factor = max(0.5, 1.0 - (avg_s_min - 0.5) * 0.3)\n            print(f\"    üîß Squeeze: factor={squeeze_factor:.2f}\")\n            mse_floor *= squeeze_factor\n            kl_floor_low = min(kl_floor_low + (50 - kl_floor_low) * (1 - squeeze_factor), 50)\n            kl_optimum = min(kl_optimum + (80 - kl_optimum) * (1 - squeeze_factor), 80)\n            kl_floor_high = max(kl_floor_high - (kl_floor_high - 150) * (1 - squeeze_factor), 150)\n\n    return history"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Full Comparison\n",
    "\n",
    "**Training time:**\n",
    "- L4: ~3-4 hours\n",
    "- A100: ~1.5 hours\n",
    "- T4: ~5-6 hours\n",
    "\n",
    "**Quick test**: Change `N_EPOCHS = 5` for 15-minute test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "N_EPOCHS = 30  # Increased for better convergence (change to 5 for quick test)\nresults = {}\n\nprint(f\"\\n{'='*60}\\nTRAINING COMPARISON: {N_EPOCHS} EPOCHS\\n{'='*60}\\n\")\n\n# Œ≤-VAE with different Œ≤ values\nfor beta in [0.0001, 0.001, 0.01, 0.1]:\n    print(f\"\\n{'='*60}\\nŒ≤-VAE (Œ≤={beta})\\n{'='*60}\")\n    model = VAE(latent_dim=128).to(device)\n    history = train_beta_vae(model, train_loader, device, beta=beta, n_epochs=N_EPOCHS)\n    test_metrics = evaluate(model, test_loader, device, max_batches=100)\n    results[f'beta_{beta}'] = {'model': model, 'history': history, 'test': test_metrics}\n    print(f\"Test: MSE={test_metrics['mse']:.4f}, KL={test_metrics['kl']:.1f}, Sharp={test_metrics['sharp']:.4f}\")\n\n# BOM-VAE\nprint(f\"\\n{'='*60}\\nBOM-VAE (no Œ≤ tuning)\\n{'='*60}\")\nmodel_bom = VAE(latent_dim=128).to(device)\nhistory_bom = train_bom_vae(model_bom, train_loader, device, n_epochs=N_EPOCHS)\ntest_metrics_bom = evaluate(model_bom, test_loader, device, max_batches=100)\nresults['bom'] = {'model': model_bom, 'history': history_bom, 'test': test_metrics_bom}\nprint(f\"Test: MSE={test_metrics_bom['mse']:.4f}, KL={test_metrics_bom['kl']:.1f}, Sharp={test_metrics_bom['sharp']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70 + \"\\nFINAL RESULTS\\n\" + \"=\"*70)\n",
    "print(f\"{'Method':<20} {'MSE':>10} {'KL':>10} {'Sharpness':>12}\")\n",
    "print(\"-\"*70)\n",
    "for name, data in results.items():\n",
    "    t = data['test']\n",
    "    print(f\"{name:<20} {t['mse']:>10.4f} {t['kl']:>10.1f} {t['sharp']:>12.4f}\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for name, data in results.items():\n",
    "    h = data['history']\n",
    "    axes[0].plot([x['mse'] for x in h], label=name.replace('_', '='), alpha=0.8)\n",
    "    axes[1].plot([x['kl'] for x in h], label=name.replace('_', '='), alpha=0.8)\n",
    "    axes[2].plot([x['sharp'] for x in h], label=name.replace('_', '='), alpha=0.8)\n",
    "axes[0].set_title('MSE (‚Üì)'); axes[0].set_yscale('log'); axes[0].legend()\n",
    "axes[1].set_title('KL'); axes[1].legend()\n",
    "axes[2].set_title('Sharpness (‚Üë)'); axes[2].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, data in results.items():\n",
    "    t = data['test']\n",
    "    marker = 's' if 'beta' in name else 'o'\n",
    "    size = 150 if 'bom' in name else 80\n",
    "    plt.scatter(t['mse'], t['kl'], s=size, marker=marker, label=name.replace('_', '='), edgecolors='black', linewidths=2)\n",
    "plt.xlabel('MSE (‚Üì)'); plt.ylabel('KL'); plt.title('Pareto Front'); plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.savefig('/content/pareto.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "test_batch = next(iter(test_loader))[0][:8].to(device)\n",
    "fig, axes = plt.subplots(len(results) + 1, 8, figsize=(16, 2*(len(results)+1)))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(test_batch[i].cpu().permute(1,2,0)); axes[0, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original', fontsize=10)\n",
    "for row, (name, data) in enumerate(results.items(), 1):\n",
    "    data['model'].eval()\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = data['model'](test_batch)\n",
    "    for i in range(8):\n",
    "        axes[row, i].imshow(recon[i].cpu().permute(1,2,0)); axes[row, i].axis('off')\n",
    "    axes[row, 0].set_ylabel(name.replace('_', '='), fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/reconstructions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results\n",
    "\n",
    "All plots are saved in `/content/`. Use the Files panel (üìÅ) on the left to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download all plots\n",
    "files.download('/content/training_comparison.png')\n",
    "files.download('/content/pareto.png')\n",
    "files.download('/content/reconstructions.png')\n",
    "\n",
    "print(\"‚úì All plots downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Œ≤-VAE** requires tuning Œ≤ to balance objectives\n",
    "   - Different Œ≤ values give different tradeoffs\n",
    "   - Low Œ≤: Good MSE, unstable KL\n",
    "   - High Œ≤: Controlled KL, poor reconstruction\n",
    "\n",
    "2. **BOM-VAE** automatically finds balanced solution\n",
    "   - No Œ≤ hyperparameter to tune\n",
    "   - Adaptive squeeze finds Pareto frontier\n",
    "   - Auto-recovery handles KL explosions\n",
    "\n",
    "3. **The key insight**: BOM optimizes the WORST objective at each step, preventing any single objective from being sacrificed."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}